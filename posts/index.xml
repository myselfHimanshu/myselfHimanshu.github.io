<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Data Summit</title>
    <link>https://myselfHimanshu.github.io/posts/</link>
    <description>Recent content in Posts on Data Summit</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 23 Feb 2020 22:22:22 +0530</lastBuildDate>
    
	<atom:link href="https://myselfHimanshu.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Sequence Model Part 3 : Sequence models &amp; Attention mechanism</title>
      <link>https://myselfHimanshu.github.io/posts/sequence_learning_3/</link>
      <pubDate>Sun, 23 Feb 2020 22:22:22 +0530</pubDate>
      
      <guid>https://myselfHimanshu.github.io/posts/sequence_learning_3/</guid>
      <description>Various Sequence to Sequence Architecture Basic Model
Let&amp;rsquo;s start with a machine translation model of converting french to english: Given a sequence X we need the output y. Here we have a network called as encoder with gru or lstm blocks which feeds in french words one at a time and outputs a vector that will represent our input french sentence. This vector can be feeded to another network called as decoder and can be trained to output the translated sentence one word at a time.</description>
    </item>
    
    <item>
      <title>Sequence Model Part 2 : Word Embeddings</title>
      <link>https://myselfHimanshu.github.io/posts/sequence_learning_2/</link>
      <pubDate>Sun, 16 Feb 2020 22:22:22 +0530</pubDate>
      
      <guid>https://myselfHimanshu.github.io/posts/sequence_learning_2/</guid>
      <description>Natural Language Processing and Word Embeddings Word Embeddings Word Representations
Word Embeddings is the way of representing the words. It lets us understand the analogies between words like (&amp;ldquo;king&amp;rdquo; and &amp;ldquo;queen&amp;rdquo;) or (&amp;ldquo;man&amp;rdquo; and &amp;ldquo;woman&amp;rdquo;).
In the previous post we represented the words with a one-hot vector. One of the weakness of one-hot vector representation is that it treats a word as a thing and doesn&amp;rsquo;t allow to generalize across the words.</description>
    </item>
    
    <item>
      <title>Sequence Model Part 1 : RNNS</title>
      <link>https://myselfHimanshu.github.io/posts/sequence_learning_1/</link>
      <pubDate>Sun, 09 Feb 2020 22:22:22 +0530</pubDate>
      
      <guid>https://myselfHimanshu.github.io/posts/sequence_learning_1/</guid>
      <description>Recurrent Neural Networks Some Examples
   Example X (input) Y(output) Type     Speech Recognition wave sequence text sequence sequence to sequence   Music Generation nothing or integer wave sequence one to sequence   Sentiment Classification text sequence integer(label) sequence to one   Machine Translation text sequence text sequence sequence to sequence   Video Activity Recognition video frames label(activity) sequence to one    All these problems have different types of input and output formats.</description>
    </item>
    
    <item>
      <title>Setting Up Paperspace for Deep Learning</title>
      <link>https://myselfHimanshu.github.io/posts/setting_paperspace_dl/</link>
      <pubDate>Sun, 02 Feb 2020 22:22:22 +0530</pubDate>
      
      <guid>https://myselfHimanshu.github.io/posts/setting_paperspace_dl/</guid>
      <description>Step by Step Guide to Setup Paperspace Machine for Deep Learning  Create an Account : paperspace.com Sign up with my promo code for Paperspace to get a $10 credit! Add in credit card information (required, even if you have a promo code) Go to https://www.paperspace.com/console/machines and click New Machine. Choose a region near to you. Choose Ubuntu 16.04 in Linux Templates. Create new P4000 machine with at least 50 GB storage and a public IP and turn off Auto Snapshot (just for saving).</description>
    </item>
    
  </channel>
</rss>