<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="https://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.55.6" />

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>Data Summit</title>
  <meta name="description" content="Everything started as nothing." />

  
  <link type="text/css" rel="stylesheet" href="https://myselfHimanshu.github.io/css/print.css" media="print">
  <link type="text/css" rel="stylesheet" href="https://myselfHimanshu.github.io/css/poole.css">
  <link type="text/css" rel="stylesheet" href="https://myselfHimanshu.github.io/css/syntax.css">
  <link type="text/css" rel="stylesheet" href="https://myselfHimanshu.github.io/css/hyde.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">


  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  <link href="https://myselfHimanshu.github.io/index.xml" rel="alternate" type="application/rss+xml" title="Data Summit" />
  

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']],
        processEscapes: true,
        processEnvironments: true,
        menuSettings: { zoom: "Double-Click" },
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        TeX: { equationNumbers: { autoNumber: "AMS" },
             extensions: ["AMSmath.js", "AMSsymbols.js"] }
      }
    });
    </script>
    
    <script type="text/javascript"
      src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>

  <link rel="stylesheet" href="https://myselfHimanshu.github.io/css/override.css">
</head>

  <body class=" ">
  <aside class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <a href="https://myselfHimanshu.github.io/"><h1>Data Summit</h1></a>
      <p class="lead">
       Everything started as nothing. 
      </p>
    </div>

    <nav>
      <ul class="sidebar-nav">
        <li><a href="https://myselfHimanshu.github.io/">Home</a> </li>
        <li><a href="https://github.com/myselfhimanshu/"> Github </a></li><li><a href="https://www.linkedin.com/in/047himanshu/"> LinkedIn </a></li><li><a href="https://twitter.com/047himanshu"> Twitter </a></li>
      </ul>
    </nav>

    <p>&copy; 2020. All rights reserved. </p>
  </div>
</aside>

    <main class="content container">
    <div class="posts">
<article class="post">
  <h1 class="post-title">
    <a href="https://myselfHimanshu.github.io/posts/cnn_01/">Convolutional Neural Network Part 1 : Basis Concepts</a>
  </h1>
  <time datetime="2020-03-22T22:22:22&#43;0530" class="post-date">Sun, Mar 22, 2020</time>
  I am starting this series of posts, where in we&rsquo;ll build on background knowledge of neural networks and explore what CNNs are. We will cover from basic understanding of how and what is CNN, augumentation techniques, architectures of CNNs, training an object detection model and more. Before moving forward, I recommend you to learn some basic terminologies of neural networks and how they work.
 If you&rsquo;re offered a seat on a rocket ship, don&rsquo;t ask what seat!
  
  <div class="read-more-link">
    <a href="/posts/cnn_01/">Read More…</a>
  </div>
  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://myselfHimanshu.github.io/posts/sequence_learning_3/">Sequence Model Part 3 : Sequence models &amp; Attention mechanism</a>
  </h1>
  <time datetime="2019-03-04T22:22:22&#43;0530" class="post-date">Mon, Mar 4, 2019</time>
  Various Sequence to Sequence Architecture Basic Model
Let&rsquo;s start with a machine translation model of converting french to english: Given a sequence X we need the output y. Here we have a network called as encoder with gru or lstm blocks which feeds in french words one at a time and outputs a vector that will represent our input french sentence. This vector can be feeded to another network called as decoder and can be trained to output the translated sentence one word at a time.
  
  <div class="read-more-link">
    <a href="/posts/sequence_learning_3/">Read More…</a>
  </div>
  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://myselfHimanshu.github.io/posts/sequence_learning_2/">Sequence Model Part 2 : Word Embeddings</a>
  </h1>
  <time datetime="2019-02-22T22:22:22&#43;0530" class="post-date">Fri, Feb 22, 2019</time>
  Natural Language Processing and Word Embeddings Word Embeddings Word Representations
Word Embeddings is the way of representing the words. It lets us understand the analogies between words like (&ldquo;king&rdquo; and &ldquo;queen&rdquo;) or (&ldquo;man&rdquo; and &ldquo;woman&rdquo;).
In the previous post we represented the words with a one-hot vector. One of the weakness of one-hot vector representation is that it treats a word as a thing and doesn&rsquo;t allow to generalize across the words.
  
  <div class="read-more-link">
    <a href="/posts/sequence_learning_2/">Read More…</a>
  </div>
  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://myselfHimanshu.github.io/posts/sequence_learning_1/">Sequence Model Part 1 : RNNS</a>
  </h1>
  <time datetime="2019-02-12T22:22:22&#43;0530" class="post-date">Tue, Feb 12, 2019</time>
  Recurrent Neural Networks Some Examples
   Example X (input) Y(output) Type     Speech Recognition wave sequence text sequence sequence to sequence   Music Generation nothing or integer wave sequence one to sequence   Sentiment Classification text sequence integer(label) sequence to one   Machine Translation text sequence text sequence sequence to sequence   Video Activity Recognition video frames label(activity) sequence to one    All these problems have different types of input and output formats.
  
  <div class="read-more-link">
    <a href="/posts/sequence_learning_1/">Read More…</a>
  </div>
  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://myselfHimanshu.github.io/posts/setting_paperspace_dl/">Setting Up Paperspace for Deep Learning</a>
  </h1>
  <time datetime="2019-02-02T22:22:22&#43;0530" class="post-date">Sat, Feb 2, 2019</time>
  Step by Step Guide to Setup Paperspace Machine for Deep Learning  Create an Account : paperspace.com Sign up with my promo code for Paperspace to get a $10 credit! Add in credit card information (required, even if you have a promo code) Go to https://www.paperspace.com/console/machines and click New Machine. Choose a region near to you. Choose Ubuntu 16.04 in Linux Templates. Create new P4000 machine with at least 50 GB storage and a public IP and turn off Auto Snapshot (just for saving).
  
  <div class="read-more-link">
    <a href="/posts/setting_paperspace_dl/">Read More…</a>
  </div>
  
</article>
</div>
    </main>

    
      
    
  </body>
</html>
